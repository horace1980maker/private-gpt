# The default configuration file.
# More information about configuration can be found in the documentation: https://docs.privategpt.dev/
# Syntax in `private_pgt/settings/settings.py`
server:
  env_name: ${APP_ENV:prod}
  port: ${PORT:8001}
  cors:
    enabled: true
    allow_origins: ["*"]
    allow_methods: ["*"]
    allow_headers: ["*"]
  auth:
    enabled: false
    # python -c 'import base64; print("Basic " + base64.b64encode("secret:key".encode()).decode())'
    # 'secret' is the username and 'key' is the password for basic auth by default
    # If the auth is enabled, this value must be set in the "Authorization" header of the request.
    secret: "Basic c2VjcmV0OmtleQ=="

data:
  local_ingestion:
    enabled: ${LOCAL_INGESTION_ENABLED:false}
    allow_ingest_from: ["*"]
  local_data_folder: local_data/private_gpt

ui:
  enabled: true
  path: /
  # "RAG", "Search", "Basic", or "Summarize"
  default_mode: "RAG"
  default_chat_system_prompt: >
    You are a Senior MEAL (Monitoring, Evaluation, Accountability, and Learning) Expert and RBM (Results-Based Management) Specialist.
    Eres un experto Senior en MEAL y Gestión Basada en Resultados (RBM).
    
    Your goal is to help design, evaluate, and improve indicators and organizational strategies.
    Tu objetivo es ayudar a diseñar, evaluar y mejorar indicadores y estrategias organizacionales.
    
    When given an indicator or document:
    1. Evaluate it using quality criteria (SMART, relevance, feasibility).
    2. Provide constructive recommendations and improved versions.
    3. Ensure definitions are technical and precise (PIRS-style).
    
    IMPORTANT LANGUAGE RULE: Detect the language of the user's message.
    - If the user writes in English, respond entirely in English.
    - If the user writes in Spanish, respond entirely in Spanish.
    - Si el usuario escribe en español, responde completamente en español.
    
  default_query_system_prompt: >
    You are a MEAL data analyst. Answer questions based only on the provided context (audits, strategic plans, indicators).
    Eres un analista de datos MEAL. Responde preguntas basándote solo en el contexto proporcionado.
    
    If the answer is not in the context, say so clearly but suggest what type of MEAL document would be needed (e.g., a PMP or baseline).
    Si la respuesta no está en el contexto, indícalo y sugiere qué documento MEAL faltaría.
    
    IMPORTANT LANGUAGE RULE: Detect the language of the user's message.
    - If the user writes in English, respond entirely in English.
    - If the user writes in Spanish, respond entirely in Spanish.
    
  default_summarization_system_prompt: >
    Provide a comprehensive summary of the provided context information.
    Proporciona un resumen completo de la información del contexto.
    
    The summary should cover all key points and main ideas, condensing information into a concise format.
    El resumen debe cubrir todos los puntos clave, condensando la información de forma concisa.
    
    IMPORTANT LANGUAGE RULE: Detect the language of the user's instructions.
    - If the user writes in English, respond entirely in English.
    - If the user writes in Spanish, respond entirely in Spanish.
  delete_file_button_enabled: true
  delete_all_files_button_enabled: true

llm:
  mode: ollama
  prompt_style: "llama3"
  # Should be matching the selected model
  max_new_tokens: 512
  context_window: 3900
  # Select your tokenizer. Llama-index tokenizer is the default.
  # tokenizer: meta-llama/Meta-Llama-3.1-8B-Instruct
  temperature: 0.1      # The temperature of the model. Increasing the temperature will make the model answer more creatively. A value of 0.1 would be more factual. (Default: 0.1)

rag:
  similarity_top_k: 2
  #This value controls how many "top" documents the RAG returns to use in the context.
  #similarity_value: 0.45
  #This value is disabled by default.  If you enable this settings, the RAG will only use articles that meet a certain percentage score.
  rerank:
    enabled: false
    model: cross-encoder/ms-marco-MiniLM-L-2-v2
    top_n: 1

summarize:
  use_async: true

clickhouse:
    host: localhost
    port: 8443
    username: admin
    password: clickhouse
    database: embeddings

llamacpp:
  llm_hf_repo_id: lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF
  llm_hf_model_file: Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf
  tfs_z: 1.0            # Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting
  top_k: 40             # Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40)
  top_p: 1.0            # Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)
  repeat_penalty: 1.1   # Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1)

embedding:
  # Should be matching the value above in most cases
  mode: ollama
  ingest_mode: simple
  embed_dim: 768 # 768 is for nomic-ai/nomic-embed-text-v1.5

huggingface:
  embedding_hf_model_name: nomic-ai/nomic-embed-text-v1.5
  access_token: ${HF_TOKEN:}
  # Warning: Enabling this option will allow the model to download and execute code from the internet.
  # Nomic AI requires this option to be enabled to use the model, be aware if you are using a different model.
  trust_remote_code: true

vectorstore:
  database: qdrant

nodestore:
  database: simple

milvus:
  uri: local_data/private_gpt/milvus/milvus_local.db
  collection_name: milvus_db
  overwrite: false

qdrant:
  path: local_data/private_gpt/qdrant

postgres:
  host: localhost
  port: 5432
  database: postgres
  user: postgres
  password: postgres
  schema_name: private_gpt

sagemaker:
  llm_endpoint_name: huggingface-pytorch-tgi-inference-2023-09-25-19-53-32-140
  embedding_endpoint_name: huggingface-pytorch-inference-2023-11-03-07-41-36-479

openai:
  api_key: ${OPENAI_API_KEY:}
  model: gpt-3.5-turbo
  embedding_api_key: ${OPENAI_API_KEY:}

ollama:
  llm_model: llama3.2
  embedding_model: nomic-embed-text
  api_base: http://localhost:11434
  embedding_api_base: http://localhost:11434  # change if your embedding model runs on another ollama
  keep_alive: 5m
  request_timeout: 300.0
  autopull_models: true

azopenai:
  api_key: ${AZ_OPENAI_API_KEY:}
  azure_endpoint: ${AZ_OPENAI_ENDPOINT:}
  embedding_deployment_name: ${AZ_OPENAI_EMBEDDING_DEPLOYMENT_NAME:}
  llm_deployment_name: ${AZ_OPENAI_LLM_DEPLOYMENT_NAME:}
  api_version: "2023-05-15"
  embedding_model: text-embedding-ada-002
  llm_model: gpt-35-turbo

gemini:
  api_key: ${GOOGLE_API_KEY:}
  model: models/gemini-pro
  embedding_model: models/embedding-001
